{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ACOLtkT5osWl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "with open('sanskrit_nlp_analysis.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sentences = [item[\"original_sentence\"] for item in data[\"sentences\"]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Sanskrit Sentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De2Fx7g7rHLy",
        "outputId": "04451a31-9ffb-4d8b-dcbb-66dd4474a4d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sanskrit Sentences:\n",
            "1. गच्छति ग्रामम्।\n",
            "2. धर्मक्षेत्रे कुरुक्षेत्रे समवेता युयुत्सवः।\n",
            "3. मामकाः पाण्डवाश्चैव किमकुर्वत सञ्जय।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_file = \"sanskrit_sentences.txt\"\n",
        "with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "    for sentence in sentences:\n",
        "        f.write(sentence + '\\n')"
      ],
      "metadata": {
        "id": "1B-Wa4xbrHOH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50\n",
        "model_prefix = \"sanskrit_bpe\"\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=temp_file,\n",
        "    model_prefix=model_prefix,\n",
        "    vocab_size=vocab_size,\n",
        "    character_coverage=1.0,\n",
        "    model_type='bpe'\n",
        ")"
      ],
      "metadata": {
        "id": "Dd-cL6JfrHP4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{model_prefix}.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TykPwjz3rOC0",
        "outputId": "2421a809-c7d0-42c9-d7f8-f988350d3a6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"\\n{i}. ORIGINAL: {sentence}\")\n",
        "    bpe_tokens = sp.encode_as_pieces(sentence)\n",
        "    print(f\"   BPE TOKENS: {bpe_tokens}\")\n",
        "\n",
        "for file in [temp_file, f\"{model_prefix}.model\", f\"{model_prefix}.vocab\"]:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_29yf0TrOGZ",
        "outputId": "84fc343a-0a41-4bb1-cdc5-97eaeeac00fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. ORIGINAL: गच्छति ग्रामम्।\n",
            "   BPE TOKENS: ['▁ग', 'च्', 'छत', 'ि', '▁ग', '्', 'र', 'ा', 'म', 'म', '्', '।']\n",
            "\n",
            "2. ORIGINAL: धर्मक्षेत्रे कुरुक्षेत्रे समवेता युयुत्सवः।\n",
            "   BPE TOKENS: ['▁', 'ध', 'र्', 'मक', '्षेत्रे', '▁क', 'ु', 'र', 'ु', 'क', '्षेत्रे', '▁स', 'म', 'व', 'े', 'ता', '▁', 'यु', 'यु', 'त्', 'स', 'व', 'ः', '।']\n",
            "\n",
            "3. ORIGINAL: मामकाः पाण्डवाश्चैव किमकुर्वत सञ्जय।\n",
            "   BPE TOKENS: ['▁', 'म', 'ा', 'मक', 'ा', 'ः', '▁', 'प', 'ा', 'ण्', 'डव', 'ा', 'श', '्', 'चै', 'व', '▁क', 'ि', 'मक', 'ु', 'र्', 'व', 'त', '▁स', 'ञ्', 'जय', '।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    bpe_tokens = sp.encode_as_pieces(sentence)\n",
        "    words = sentence.replace('।', '').split()\n",
        "\n",
        "    current_word_tokens = []\n",
        "    word_mappings = []\n",
        "\n",
        "    for token in bpe_tokens:\n",
        "        if token.startswith('▁') and current_word_tokens:\n",
        "            if current_word_tokens:\n",
        "                word_mappings.append(current_word_tokens)\n",
        "                current_word_tokens = []\n",
        "            current_word_tokens.append(token[1:])\n",
        "        elif token == '▁':\n",
        "            if current_word_tokens:\n",
        "                word_mappings.append(current_word_tokens)\n",
        "                current_word_tokens = []\n",
        "        else:\n",
        "            current_word_tokens.append(token)\n",
        "\n",
        "    if current_word_tokens:\n",
        "        word_mappings.append(current_word_tokens)\n",
        "\n",
        "    for word_idx, (word, tokens) in enumerate(zip(words, word_mappings)):\n",
        "        print(f\"     '{word}' → {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ2Biy9XrT-E",
        "outputId": "fb2c6318-2a5e-4ec1-c8b1-8ef4817346c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     'गच्छति' → ['▁ग', 'च्', 'छत', 'ि']\n",
            "     'ग्रामम्' → ['ग', '्', 'र', 'ा', 'म', 'म', '्', '।']\n",
            "     'धर्मक्षेत्रे' → ['ध', 'र्', 'मक', '्षेत्रे']\n",
            "     'कुरुक्षेत्रे' → ['क', 'ु', 'र', 'ु', 'क', '्षेत्रे']\n",
            "     'समवेता' → ['स', 'म', 'व', 'े', 'ता']\n",
            "     'युयुत्सवः' → ['', 'यु', 'यु', 'त्', 'स', 'व', 'ः', '।']\n",
            "     'मामकाः' → ['म', 'ा', 'मक', 'ा', 'ः']\n",
            "     'पाण्डवाश्चैव' → ['', 'प', 'ा', 'ण्', 'डव', 'ा', 'श', '्', 'चै', 'व']\n",
            "     'किमकुर्वत' → ['क', 'ि', 'मक', 'ु', 'र्', 'व', 'त']\n",
            "     'सञ्जय' → ['स', 'ञ्', 'जय', '।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "token_vocab_size = 1000\n",
        "token_emb_dim = 256\n",
        "\n",
        "pos_vocab_size = 50\n",
        "pos_emb_dim = 32\n",
        "\n",
        "deprel_vocab_size = 50\n",
        "deprel_emb_dim = 16\n",
        "\n",
        "root_vocab_size = 5000\n",
        "root_emb_dim = 64\n",
        "\n",
        "synset_vocab_size = 10000\n",
        "synset_emb_dim = 64\n",
        "\n",
        "token_embeddings = nn.Embedding(token_vocab_size, token_emb_dim)\n",
        "pos_embeddings = nn.Embedding(pos_vocab_size, pos_emb_dim)\n",
        "deprel_embeddings = nn.Embedding(deprel_vocab_size, deprel_emb_dim)\n",
        "root_embeddings = nn.Embedding(root_vocab_size, root_emb_dim)\n",
        "synset_embeddings = nn.Embedding(synset_vocab_size, synset_emb_dim)\n"
      ],
      "metadata": {
        "id": "V44OxX_LrHTV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_vocabularies(data):\n",
        "    token_vocab = defaultdict(lambda: len(token_vocab))\n",
        "    pos_vocab = defaultdict(lambda: len(pos_vocab))\n",
        "    deprel_vocab = defaultdict(lambda: len(deprel_vocab))\n",
        "    root_vocab = defaultdict(lambda: len(root_vocab))\n",
        "    synset_vocab = defaultdict(lambda: len(synset_vocab))\n",
        "\n",
        "    token_vocab['<UNK>'] = 0\n",
        "    pos_vocab['<UNK>'] = 0\n",
        "    deprel_vocab['<UNK>'] = 0\n",
        "    root_vocab['<UNK>'] = 0\n",
        "    synset_vocab['<UNK>'] = 0\n",
        "\n",
        "    for sentence in data['sentences']:\n",
        "        for annotation in sentence['annotation']:\n",
        "            # Token text\n",
        "            token_vocab[annotation['text']]\n",
        "\n",
        "            # POS tags\n",
        "            pos_vocab[annotation['pos']]\n",
        "            pos_vocab[annotation['upos']]\n",
        "\n",
        "            # Dependency relations\n",
        "            deprel_vocab[annotation['deprel']]\n",
        "\n",
        "            # Roots from word analysis\n",
        "            for word_analysis in annotation['word_analysis']:\n",
        "                root_vocab[word_analysis['root']]\n",
        "\n",
        "                # Synsets\n",
        "                for synset_item in word_analysis['synset_data']:\n",
        "                    synset_vocab[synset_item['synset']]\n",
        "\n",
        "    return (dict(token_vocab), dict(pos_vocab), dict(deprel_vocab),\n",
        "            dict(root_vocab), dict(synset_vocab))\n"
      ],
      "metadata": {
        "id": "UtLWSCOWruXI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_Cb7PvbsHXg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "token_vocab, pos_vocab, deprel_vocab, root_vocab, synset_vocab = build_vocabularies(data)"
      ],
      "metadata": {
        "id": "Bkh7uclfr4al"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_emb_dim = 256\n",
        "pos_emb_dim = 32\n",
        "deprel_emb_dim = 16\n",
        "root_emb_dim = 64\n",
        "synset_emb_dim = 64\n",
        "\n",
        "token_embeddings = nn.Embedding(len(token_vocab), token_emb_dim)\n",
        "pos_embeddings = nn.Embedding(len(pos_vocab), pos_emb_dim)\n",
        "deprel_embeddings = nn.Embedding(len(deprel_vocab), deprel_emb_dim)\n",
        "root_embeddings = nn.Embedding(len(root_vocab), root_emb_dim)\n",
        "synset_embeddings = nn.Embedding(len(synset_vocab), synset_emb_dim)"
      ],
      "metadata": {
        "id": "nO4AVUIfsKfX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentence_embeddings(sentence_annotation, vocab_dicts):\n",
        "    token_vocab, pos_vocab, deprel_vocab, root_vocab, synset_vocab = vocab_dicts\n",
        "\n",
        "    all_token_embs = []\n",
        "    all_pos_embs = []\n",
        "    all_deprel_embs = []\n",
        "    all_root_embs = []\n",
        "    all_synset_embs = []\n",
        "\n",
        "    for annotation in sentence_annotation:\n",
        "        token_idx = token_vocab.get(annotation['text'], 0)  # 0 is UNK\n",
        "        token_emb = token_embeddings(torch.tensor(token_idx))\n",
        "        all_token_embs.append(token_emb)\n",
        "\n",
        "        pos_idx = pos_vocab.get(annotation['pos'], 0)\n",
        "        upos_idx = pos_vocab.get(annotation['upos'], 0)\n",
        "        pos_emb = pos_embeddings(torch.tensor(pos_idx))\n",
        "        upos_emb = pos_embeddings(torch.tensor(upos_idx))\n",
        "        all_pos_embs.extend([pos_emb, upos_emb])\n",
        "\n",
        "        deprel_idx = deprel_vocab.get(annotation['deprel'], 0)\n",
        "        deprel_emb = deprel_embeddings(torch.tensor(deprel_idx))\n",
        "        all_deprel_embs.append(deprel_emb)\n",
        "\n",
        "        for word_analysis in annotation['word_analysis']:\n",
        "            root_idx = root_vocab.get(word_analysis['root'], 0)\n",
        "            root_emb = root_embeddings(torch.tensor(root_idx))\n",
        "            all_root_embs.append(root_emb)\n",
        "\n",
        "            for synset_item in word_analysis['synset_data']:\n",
        "                synset_idx = synset_vocab.get(synset_item['synset'], 0)\n",
        "                synset_emb = synset_embeddings(torch.tensor(synset_idx))\n",
        "                all_synset_embs.append(synset_emb)\n",
        "\n",
        "    token_embs_stack = torch.stack(all_token_embs) if all_token_embs else torch.tensor([])\n",
        "    pos_embs_stack = torch.stack(all_pos_embs) if all_pos_embs else torch.tensor([])\n",
        "    deprel_embs_stack = torch.stack(all_deprel_embs) if all_deprel_embs else torch.tensor([])\n",
        "    root_embs_stack = torch.stack(all_root_embs) if all_root_embs else torch.tensor([])\n",
        "    synset_embs_stack = torch.stack(all_synset_embs) if all_synset_embs else torch.tensor([])\n",
        "\n",
        "    return {\n",
        "        'token_embeddings': token_embs_stack,\n",
        "        'pos_embeddings': pos_embs_stack,\n",
        "        'deprel_embeddings': deprel_embs_stack,\n",
        "        'root_embeddings': root_embs_stack,\n",
        "        'synset_embeddings': synset_embs_stack\n",
        "    }"
      ],
      "metadata": {
        "id": "DCeYp3DPsQtp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dicts = (token_vocab, pos_vocab, deprel_vocab, root_vocab, synset_vocab)"
      ],
      "metadata": {
        "id": "uhw_KlPUsV2S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings = []\n",
        "for i, sentence in enumerate(data['sentences']):\n",
        "    print(f\"Processing sentence {i+1}: {sentence['original_sentence']}\")\n",
        "    emb_dict = create_sentence_embeddings(sentence['annotation'], vocab_dicts)\n",
        "    sentence_embeddings.append(emb_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIRROD30sXNB",
        "outputId": "cf76401b-fd83-455d-bc03-0f018549a388"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing sentence 1: गच्छति ग्रामम्।\n",
            "Processing sentence 2: धर्मक्षेत्रे कुरुक्षेत्रे समवेता युयुत्सवः।\n",
            "Processing sentence 3: मामकाः पाण्डवाश्चैव किमकुर्वत सञ्जय।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "with open('sanskrit_nlp_analysis.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "f3uy30YVtGAw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_vocabularies(data):\n",
        "    token_vocab = defaultdict(lambda: len(token_vocab))\n",
        "    pos_vocab = defaultdict(lambda: len(pos_vocab))\n",
        "    upos_vocab = defaultdict(lambda: len(upos_vocab))\n",
        "    deprel_vocab = defaultdict(lambda: len(deprel_vocab))\n",
        "    root_vocab = defaultdict(lambda: len(root_vocab))\n",
        "    synset_vocab = defaultdict(lambda: len(synset_vocab))\n",
        "    feats_vocab = defaultdict(lambda: len(feats_vocab))\n",
        "    category_vocab = defaultdict(lambda: len(category_vocab))\n",
        "    lemma_vocab = defaultdict(lambda: len(lemma_vocab))\n",
        "\n",
        "    token_vocab['<UNK>'] = 0\n",
        "    pos_vocab['<UNK>'] = 0\n",
        "    upos_vocab['<UNK>'] = 0\n",
        "    deprel_vocab['<UNK>'] = 0\n",
        "    root_vocab['<UNK>'] = 0\n",
        "    synset_vocab['<UNK>'] = 0\n",
        "    feats_vocab['<UNK>'] = 0\n",
        "    category_vocab['<UNK>'] = 0\n",
        "    lemma_vocab['<UNK>'] = 0\n",
        "\n",
        "    for sentence in data['sentences']:\n",
        "        for annotation in sentence['annotation']:\n",
        "            if annotation['text']:\n",
        "                token_vocab[annotation['text']]\n",
        "            if annotation['pos']:\n",
        "                pos_vocab[annotation['pos']]\n",
        "            if annotation['upos']:\n",
        "                upos_vocab[annotation['upos']]\n",
        "            if annotation['deprel']:\n",
        "                deprel_vocab[annotation['deprel']]\n",
        "            if annotation['feats']:\n",
        "                feats_vocab[annotation['feats']]\n",
        "\n",
        "            if annotation['word_analysis']:\n",
        "                for word_analysis in annotation['word_analysis']:\n",
        "                    if word_analysis['root']:\n",
        "                        root_vocab[word_analysis['root']]\n",
        "                    if word_analysis['category']:\n",
        "                        for category in word_analysis['category']:\n",
        "                            category_vocab[category]\n",
        "                    if word_analysis['synset_data']:\n",
        "                        for synset_item in word_analysis['synset_data']:\n",
        "                            if synset_item['synset']:\n",
        "                                synset_vocab[synset_item['synset']]\n",
        "                            if synset_item['lemma_names']:\n",
        "                                for lemma in synset_item['lemma_names']:\n",
        "                                    lemma_vocab[lemma]\n",
        "\n",
        "    return (dict(token_vocab), dict(pos_vocab), dict(upos_vocab),\n",
        "            dict(deprel_vocab), dict(root_vocab), dict(synset_vocab),\n",
        "            dict(feats_vocab), dict(category_vocab), dict(lemma_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfs964hmv2Jc",
        "outputId": "e6841eb3-a04c-4e29-b653-5e8302e7ff12"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Sizes:\n",
            "Token vocab: 11\n",
            "POS vocab: 4\n",
            "UPOS vocab: 4\n",
            "Deprel vocab: 4\n",
            "Root vocab: 13\n",
            "Synset vocab: 8\n",
            "Feats vocab: 5\n",
            "Category vocab: 7\n",
            "Lemma vocab: 69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_dicts = build_vocabularies(data)\n",
        "token_vocab, pos_vocab, upos_vocab, deprel_vocab, root_vocab, synset_vocab, feats_vocab, category_vocab, lemma_vocab = vocab_dicts\n",
        "\n",
        "token_emb_dim = 256\n",
        "pos_emb_dim = 32\n",
        "upos_emb_dim = 32\n",
        "deprel_emb_dim = 16\n",
        "root_emb_dim = 64\n",
        "synset_emb_dim = 64\n",
        "feats_emb_dim = 32\n",
        "category_emb_dim = 32\n",
        "lemma_emb_dim = 64\n"
      ],
      "metadata": {
        "id": "igF8E5bnv59H"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "token_embeddings = nn.Embedding(len(token_vocab), token_emb_dim)\n",
        "pos_embeddings = nn.Embedding(len(pos_vocab), pos_emb_dim)\n",
        "upos_embeddings = nn.Embedding(len(upos_vocab), upos_emb_dim)\n",
        "deprel_embeddings = nn.Embedding(len(deprel_vocab), deprel_emb_dim)\n",
        "root_embeddings = nn.Embedding(len(root_vocab), root_emb_dim)\n",
        "synset_embeddings = nn.Embedding(len(synset_vocab), synset_emb_dim)\n",
        "feats_embeddings = nn.Embedding(len(feats_vocab), feats_emb_dim)\n",
        "category_embeddings = nn.Embedding(len(category_vocab), category_emb_dim)\n",
        "lemma_embeddings = nn.Embedding(len(lemma_vocab), lemma_emb_dim)\n"
      ],
      "metadata": {
        "id": "SACF6Q5bv9Kf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_token_embeddings(annotation, vocab_dicts):\n",
        "    (token_vocab, pos_vocab, upos_vocab, deprel_vocab, root_vocab,\n",
        "     synset_vocab, feats_vocab, category_vocab, lemma_vocab) = vocab_dicts\n",
        "\n",
        "    token_idx = token_vocab.get(annotation['text'], 0) if annotation['text'] else 0\n",
        "    pos_idx = pos_vocab.get(annotation['pos'], 0) if annotation['pos'] else 0\n",
        "    upos_idx = upos_vocab.get(annotation['upos'], 0) if annotation['upos'] else 0\n",
        "    deprel_idx = deprel_vocab.get(annotation['deprel'], 0) if annotation['deprel'] else 0\n",
        "    feats_idx = feats_vocab.get(annotation['feats'], 0) if annotation['feats'] else 0\n",
        "\n",
        "    token_emb = token_embeddings(torch.tensor(token_idx))\n",
        "    pos_emb = pos_embeddings(torch.tensor(pos_idx))\n",
        "    upos_emb = upos_embeddings(torch.tensor(upos_idx))\n",
        "    deprel_emb = deprel_embeddings(torch.tensor(deprel_idx))\n",
        "    feats_emb = feats_embeddings(torch.tensor(feats_idx))\n",
        "\n",
        "    root_embs = []\n",
        "    category_embs = []\n",
        "    synset_embs = []\n",
        "    lemma_embs = []\n",
        "\n",
        "    if annotation['word_analysis']:\n",
        "        for word_analysis in annotation['word_analysis']:\n",
        "            if word_analysis['root']:\n",
        "                root_idx = root_vocab.get(word_analysis['root'], 0)\n",
        "                root_emb = root_embeddings(torch.tensor(root_idx))\n",
        "                root_embs.append(root_emb)\n",
        "\n",
        "            if word_analysis['category']:\n",
        "                for category in word_analysis['category']:\n",
        "                    category_idx = category_vocab.get(category, 0)\n",
        "                    category_emb = category_embeddings(torch.tensor(category_idx))\n",
        "                    category_embs.append(category_emb)\n",
        "\n",
        "            if word_analysis['synset_data']:\n",
        "                for synset_item in word_analysis['synset_data']:\n",
        "                    if synset_item['synset']:\n",
        "                        synset_idx = synset_vocab.get(synset_item['synset'], 0)\n",
        "                        synset_emb = synset_embeddings(torch.tensor(synset_idx))\n",
        "                        synset_embs.append(synset_emb)\n",
        "\n",
        "                    if synset_item['lemma_names']:\n",
        "                        for lemma in synset_item['lemma_names']:\n",
        "                            lemma_idx = lemma_vocab.get(lemma, 0)\n",
        "                            lemma_emb = lemma_embeddings(torch.tensor(lemma_idx))\n",
        "                            lemma_embs.append(lemma_emb)\n",
        "\n",
        "    avg_root_emb = torch.mean(torch.stack(root_embs), dim=0) if root_embs else torch.zeros(root_emb_dim)\n",
        "    avg_category_emb = torch.mean(torch.stack(category_embs), dim=0) if category_embs else torch.zeros(category_emb_dim)\n",
        "    avg_synset_emb = torch.mean(torch.stack(synset_embs), dim=0) if synset_embs else torch.zeros(synset_emb_dim)\n",
        "    avg_lemma_emb = torch.mean(torch.stack(lemma_embs), dim=0) if lemma_embs else torch.zeros(lemma_emb_dim)\n",
        "\n",
        "    return {\n",
        "        'text': annotation['text'],\n",
        "        'token_emb': token_emb,\n",
        "        'pos_emb': pos_emb,\n",
        "        'upos_emb': upos_emb,\n",
        "        'deprel_emb': deprel_emb,\n",
        "        'feats_emb': feats_emb,\n",
        "        'root_emb': avg_root_emb,\n",
        "        'category_emb': avg_category_emb,\n",
        "        'synset_emb': avg_synset_emb,\n",
        "        'lemma_emb': avg_lemma_emb\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEo72tT1wAnX",
        "outputId": "7653a4de-4126-44db-b6e3-2338584f0b5e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " processed sentence 1: 'गच्छति ग्रामम्।'\n",
            "================================================================================\n",
            "['स', 'ञ्', 'जय', '।']\n",
            "\n",
            " processed sentence 2: 'धर्मक्षेत्रे कुरुक्षेत्रे समवेता युयुत्सवः।'\n",
            "================================================================================\n",
            "['स', 'ञ्', 'जय', '।']\n",
            "\n",
            " processed sentence 3: 'मामकाः पाण्डवाश्चैव किमकुर्वत सञ्जय।'\n",
            "================================================================================\n",
            "['स', 'ञ्', 'जय', '।']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence_embeddings_data = []\n",
        "\n",
        "for i, sentence in enumerate(data['sentences']):\n",
        "    sentence_tokens = []\n",
        "    for j, annotation in enumerate(sentence['annotation']):\n",
        "        token_emb_dict = create_token_embeddings(annotation, vocab_dicts)\n",
        "        sentence_tokens.append(token_emb_dict)\n",
        "\n",
        "    sentence_embeddings_data.append({\n",
        "        'original_sentence': sentence['original_sentence'],\n",
        "        'token_embeddings': sentence_tokens\n",
        "    })\n",
        "\n",
        "print(f\"Created embeddings for {len(sentence_embeddings_data)} sentences\")\n",
        "print(f\"First sentence tokens: {[token['text'] for token in sentence_embeddings_data[0]['token_embeddings']]}\")\n",
        "print(sentence_embeddings_data[0]['token_embeddings'][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOqEjuOdwJ4c",
        "outputId": "bbda9be2-0dfc-4473-f6b8-f3512f348952"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created embeddings for 3 sentences\n",
            "First sentence tokens: ['गच्छति', 'ग्रामम्।']\n",
            "{'text': 'ग्रामम्।', 'token_emb': tensor([-1.5419,  1.2567, -0.1201, -0.7505,  0.1122,  1.2251,  1.5011,  2.7073,\n",
            "        -1.0551,  0.3698,  0.3526,  0.4196, -2.4235,  0.9096,  0.0746, -0.0574,\n",
            "         0.7172, -1.4922,  0.7821,  0.1955,  0.6878, -1.3480,  0.2004, -2.0693,\n",
            "         1.0934,  0.2808, -1.1844,  0.4219,  0.0847,  0.9155, -1.1510, -0.7705,\n",
            "         0.1779,  0.2910, -0.0265,  0.0880,  0.2620, -2.1098,  0.6813, -1.5978,\n",
            "         0.4127, -1.2584, -1.2487,  0.4738, -1.2487,  1.8917, -0.1917, -1.0562,\n",
            "         0.1621, -0.7743,  1.5597,  1.7982, -0.4630, -0.7597,  1.0255, -1.1869,\n",
            "         0.8289,  1.0214, -0.0325, -0.3919,  2.4126, -0.4691, -0.0146, -0.4110,\n",
            "         0.7154, -1.4378,  0.3520,  1.2575,  0.6033,  1.2389,  0.7635, -1.6764,\n",
            "         1.3035, -1.6167, -0.5825,  1.5785,  0.1532,  1.4402, -0.4596, -0.3535,\n",
            "         1.3310,  1.3627, -1.4788,  0.1574,  1.8643,  0.0310, -1.2977, -1.8781,\n",
            "         0.1278,  0.3450,  0.3235, -0.5478,  0.9897, -1.2247, -0.9037, -0.7417,\n",
            "         0.6864, -0.4049,  1.0647, -0.7363,  0.6909, -1.2936,  0.2981, -0.2865,\n",
            "         0.0811, -0.2199,  0.0259,  2.9231, -1.0018, -0.1520, -0.2944, -0.5036,\n",
            "        -0.8466,  0.5390,  0.2630, -2.3178,  0.7421,  0.5030, -0.9725,  0.1196,\n",
            "         0.5082, -0.6136,  1.2192, -0.5519, -0.3726,  0.8829,  0.5604,  0.0568,\n",
            "        -2.3624,  0.1566,  0.3684,  0.7442,  1.5985, -0.1984,  0.2703, -1.0911,\n",
            "         0.1437,  0.8461, -1.0265,  1.3940, -0.2458,  0.2011, -0.0512,  0.2426,\n",
            "         0.0499, -0.7128,  0.4626,  0.6770,  1.5506, -0.0948, -2.4336,  0.2057,\n",
            "         0.3117,  0.6590,  0.6027, -1.2512, -0.7317, -1.0658, -0.1698, -0.6577,\n",
            "        -1.1502, -1.6124, -1.3192, -0.5067,  0.8574,  0.9590, -1.5791,  1.8227,\n",
            "         0.3972,  0.0559, -1.3000, -0.0143,  0.1358,  0.4543,  1.2720,  1.3970,\n",
            "         1.5152, -1.1353,  0.2591,  0.6785, -0.6584,  0.6132, -1.7289, -1.3194,\n",
            "         0.8459, -0.0588,  0.9256,  0.2478,  1.1899,  0.7966, -0.2891, -1.0379,\n",
            "         0.7283,  0.4421, -0.7662,  0.2228,  1.1410,  1.5315,  0.4821, -1.2595,\n",
            "        -0.0310,  0.4166,  0.6563,  0.2308,  1.4155,  0.9540, -1.3840, -0.8606,\n",
            "         0.6602,  1.0575, -1.6537,  2.5525, -1.2778, -0.6649, -0.9281, -1.3630,\n",
            "         1.9705,  1.4261, -0.9855, -0.6163, -0.8819,  0.0907,  0.2431,  2.3807,\n",
            "         0.4775,  0.0303,  1.6986, -0.7429,  1.2545,  1.3021, -1.2227,  1.3634,\n",
            "         0.1271,  0.7926, -1.6275, -0.1958, -1.3152,  0.3183,  1.6219, -0.5619,\n",
            "        -1.0075,  0.1321,  0.9529, -0.6683,  0.0258, -0.9331, -0.5981,  0.7836,\n",
            "        -0.4366,  0.6953, -0.2842,  1.2139,  0.2482, -0.5566, -0.3475, -1.4603],\n",
            "       grad_fn=<EmbeddingBackward0>), 'pos_emb': tensor([-0.5619, -0.8241,  0.8207,  1.5004, -0.2207, -0.6278,  0.0968,  0.4860,\n",
            "         0.0191,  1.5121, -2.5641,  0.4171,  1.8182, -1.5802,  0.8450, -1.6080,\n",
            "         0.3406, -0.8580,  0.1033, -0.3171, -0.1842,  0.0799,  0.2899, -0.4062,\n",
            "        -0.1201, -0.9965, -2.3694, -1.4229, -0.0594,  0.3045, -0.5015,  0.5683],\n",
            "       grad_fn=<EmbeddingBackward0>), 'upos_emb': tensor([ 8.0381e-01, -8.9497e-01, -2.2523e+00,  1.0178e+00,  7.6881e-02,\n",
            "        -8.9118e-01,  9.8996e-01,  8.0176e-01,  1.8894e+00,  6.0759e-01,\n",
            "        -1.3344e+00,  9.2138e-01,  5.6978e-01,  3.1023e-01,  1.6902e+00,\n",
            "        -2.5429e-01, -2.6151e+00,  8.6862e-01, -1.1713e-01, -1.3074e+00,\n",
            "         4.0089e-01, -5.9407e-01, -2.1457e-01,  2.4216e-03, -1.1369e+00,\n",
            "         5.0902e-01, -1.7448e-02, -2.0729e+00,  9.7458e-02,  1.0136e+00,\n",
            "         1.5790e+00, -4.6634e-01], grad_fn=<EmbeddingBackward0>), 'deprel_emb': tensor([ 0.4776, -0.2093,  0.7109, -0.7445,  1.1387, -0.4582, -1.1189,  1.4805,\n",
            "        -1.0018, -0.7508, -0.4860,  1.1642, -0.2553,  1.4679, -0.0119, -0.8390],\n",
            "       grad_fn=<EmbeddingBackward0>), 'feats_emb': tensor([ 0.9320,  1.5490, -1.4657,  0.2825, -0.7812,  0.7927, -0.9601, -0.8993,\n",
            "        -0.9748, -0.2641, -1.0226, -0.0083, -1.3769,  0.7364, -2.3052, -0.3598,\n",
            "         0.8360,  0.4384, -1.1138, -2.5201,  0.5481,  0.8089, -0.2511,  0.6867,\n",
            "        -0.6517, -0.9106, -0.3801,  0.7744,  0.8731, -1.4012, -0.5115, -0.0199],\n",
            "       grad_fn=<EmbeddingBackward0>), 'root_emb': tensor([ 1.0325, -1.9992,  1.8655, -1.9744,  0.6539, -0.4049, -0.7040,  0.4721,\n",
            "        -1.4316,  0.5195,  0.3985, -1.1693, -0.3999,  0.3871, -1.6520, -0.8345,\n",
            "        -1.4347,  0.3918, -0.7856,  0.7113, -0.3523,  0.8578,  0.4730, -0.0124,\n",
            "         0.0924, -0.5030,  0.3527,  0.2727,  1.7131,  1.8681, -1.0832,  1.4514,\n",
            "         1.4976, -1.0401, -2.1903, -0.9834, -0.6760,  1.2107,  0.7304,  0.3146,\n",
            "         0.6190,  1.1077,  1.8536,  0.0482, -0.8360, -0.9209,  0.5847, -0.9202,\n",
            "         0.0254, -0.9717,  1.3477, -1.5715, -0.1924,  0.2864, -0.3323, -0.2497,\n",
            "         0.0278,  0.9700,  1.1258,  0.7328, -0.3979,  0.4860, -1.6247, -1.2650],\n",
            "       grad_fn=<MeanBackward1>), 'category_emb': tensor([ 2.6460,  0.6830,  2.1852, -0.5714, -0.0953, -0.7320,  1.0224, -1.7957,\n",
            "         0.9895,  1.4633,  1.3321, -0.9065,  0.2554,  0.8228,  0.0809, -1.9775,\n",
            "        -1.0007, -1.7678,  0.4064, -0.2238,  0.7019, -1.0287, -1.4734,  1.4772,\n",
            "        -2.0124, -0.9016, -0.0785,  0.0086,  0.3475, -0.1204,  0.3230, -0.5707],\n",
            "       grad_fn=<MeanBackward1>), 'synset_emb': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'lemma_emb': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0khCGHnx0xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def get_positional_encoding(max_length, d_model):\n",
        "\n",
        "    position = np.arange(max_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    pos_encoding = np.zeros((max_length, d_model))\n",
        "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    return pos_encoding\n"
      ],
      "metadata": {
        "id": "QlOsPMhRywj7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_sentence_positional_embeddings(sentence_data, d_model=512):\n",
        "\n",
        "    words = [annotation[\"text\"].replace(\"।\", \"\") for annotation in sentence_data[\"annotation\"]]\n",
        "    max_length = len(words)\n",
        "\n",
        "    pos_encoding = get_positional_encoding(max_length, d_model)\n",
        "\n",
        "    word_positions = []\n",
        "    for i, word in enumerate(words):\n",
        "        word_positions.append({\n",
        "            \"word\": word,\n",
        "            \"position\": i,\n",
        "            \"embedding\": pos_encoding[i].tolist()\n",
        "        })\n",
        "\n",
        "    return word_positions\n"
      ],
      "metadata": {
        "id": "L4B1jUOLzBL4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_sanskrit_json(json_file_path, d_model=512):\n",
        "\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for sentence in data[\"sentences\"]:\n",
        "        original_sentence = sentence[\"original_sentence\"]\n",
        "        positional_embeddings = create_sentence_positional_embeddings(sentence, d_model)\n",
        "\n",
        "        results.append({\n",
        "            \"original_sentence\": original_sentence,\n",
        "            \"positional_embeddings\": positional_embeddings\n",
        "        })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "v9bWRyqVzBPa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    json_file_path = \"sanskrit_nlp_analysis.json\"\n",
        "\n",
        "    embeddings = process_sanskrit_json(json_file_path, d_model=128)\n",
        "\n",
        "    for i, result in enumerate(embeddings):\n",
        "        for word_embedding in result[\"positional_embeddings\"]:\n",
        "            print(f\"  {word_embedding['word']} (position {word_embedding['position']}): \"\n",
        "                  f\"embedding length = {len(word_embedding['embedding'])}\")\n",
        "\n",
        "    output_data = {\n",
        "        \"positional_embeddings\": embeddings\n",
        "    }\n",
        "\n",
        "    with open(\"sanskrit_positional_embeddings.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyDyugv5zBTA",
        "outputId": "4087823c-aa77-482a-b01e-6509953db02d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  गच्छति (position 0): embedding length = 128\n",
            "  ग्रामम् (position 1): embedding length = 128\n",
            "  धर्मक्षेत्रे (position 0): embedding length = 128\n",
            "  कुरुक्षेत्रे (position 1): embedding length = 128\n",
            "  समवेता (position 2): embedding length = 128\n",
            "  युयुत्सवः (position 3): embedding length = 128\n",
            "  मामकाः (position 0): embedding length = 128\n",
            "  पाण्डवाश्चैव (position 1): embedding length = 128\n",
            "  किमकुर्वत (position 2): embedding length = 128\n",
            "  सञ्जय (position 3): embedding length = 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "84J0W9WKy_BS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}